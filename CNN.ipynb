{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from utils.preprocess import getData\n",
    "from networks.cnn import BasicCNN\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Load, Preprocess Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_valid, y_valid, X_test, y_test = getData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shape of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: torch.Size([6960, 22, 250, 1])\n",
      "Valid data shape: torch.Size([1500, 22, 250, 1])\n",
      "Test data shape: torch.Size([1772, 22, 250, 1])\n",
      "Training target shape: torch.Size([6960, 4])\n",
      "Valid target shape: torch.Size([1500, 4])\n",
      "Test target shape: torch.Size([1772, 4])\n"
     ]
    }
   ],
   "source": [
    "print('Training data shape: {}'.format(X_train.shape))\n",
    "print('Valid data shape: {}'.format(X_valid.shape))\n",
    "print('Test data shape: {}'.format(X_test.shape))\n",
    "\n",
    "print('Training target shape: {}'.format(y_train.shape))\n",
    "print('Valid target shape: {}'.format(y_valid.shape))\n",
    "print('Test target shape: {}'.format(y_test.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Dataset and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "trainset = torch.utils.data.TensorDataset(X_train,y_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "# Shuffle is set to false for validation and test sets since no training is done on them, all we do is evaluate.\n",
    "valset =  torch.utils.data.TensorDataset(X_valid, y_valid)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=2)\n",
    "\n",
    "testset = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BasicCNN(\n",
      "  (conv1): Conv2d(22, 25, kernel_size=(10, 1), stride=(1, 1), padding=same)\n",
      "  (pool1): MaxPool2d(kernel_size=(3, 1), stride=1, padding=(1, 0), dilation=1, ceil_mode=False)\n",
      "  (batchnorm1): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): Conv2d(25, 50, kernel_size=(10, 1), stride=(1, 1), padding=same)\n",
      "  (pool2): MaxPool2d(kernel_size=(3, 1), stride=1, padding=(1, 0), dilation=1, ceil_mode=False)\n",
      "  (batchnorm2): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): Conv2d(50, 100, kernel_size=(10, 1), stride=(1, 1), padding=same)\n",
      "  (pool3): MaxPool2d(kernel_size=(3, 1), stride=1, padding=(1, 0), dilation=1, ceil_mode=False)\n",
      "  (batchnorm3): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv4): Conv2d(100, 200, kernel_size=(10, 1), stride=(1, 1), padding=same)\n",
      "  (pool4): MaxPool2d(kernel_size=(3, 1), stride=1, padding=(1, 0), dilation=1, ceil_mode=False)\n",
      "  (batchnorm4): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (affine): Linear(in_features=50000, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Initialize the network\n",
    "network = BasicCNN()\n",
    "print(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Epoch: 0 | Train Loss: 3.0468455378068695 | Train Accuracy: 52.19827586206897\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Epoch: 1 | Train Loss: 1.69135645879518 | Train Accuracy: 70.90517241379311\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Epoch: 2 | Train Loss: 0.8104951568973173 | Train Accuracy: 83.97988505747126\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Epoch: 3 | Train Loss: 0.5468514112909453 | Train Accuracy: 89.066091954023\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Epoch: 4 | Train Loss: 0.3780240286910616 | Train Accuracy: 91.72413793103448\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Epoch: 5 | Train Loss: 0.2530777700920376 | Train Accuracy: 94.6264367816092\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Epoch: 6 | Train Loss: 0.21750833532973748 | Train Accuracy: 95.43103448275862\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Epoch: 7 | Train Loss: 0.27380290343383445 | Train Accuracy: 94.29597701149426\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Epoch: 8 | Train Loss: 0.3094928629917051 | Train Accuracy: 95.0\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Iterating\n",
      "Epoch: 9 | Train Loss: 0.1826543394538121 | Train Accuracy: 96.59482758620689\n"
     ]
    }
   ],
   "source": [
    "# Select loss criterion\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.001)\n",
    "\n",
    "# Train the network\n",
    "num_epochs = 10\n",
    "\n",
    "# Store the loss\n",
    "train_accuracies = []\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(num_epochs): # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        print('Iterating')\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.float()\n",
    "        labels = labels.float()\n",
    "  \n",
    "        # forward pass\n",
    "        outputs = network(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "\n",
    "        # backward + optimize\n",
    "        loss.backward() # backward to get gradient values\n",
    "        \n",
    "        optimizer.step() # does the update\n",
    "    \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # accumulate loss\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Make prediction for batch\n",
    "        _, predicted = outputs.max(1)\n",
    "        \n",
    "        # Store accuracy for batch\n",
    "        # WE convert back from one-hot to integer for checking accuracy\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(torch.argmax(labels, dim=1)).sum().item()\n",
    "        \n",
    "    # Store accuracy,loss for epoch\n",
    "    train_loss=running_loss/len(trainloader)\n",
    "    train_accuracy=100.*correct/total\n",
    "    \n",
    "    train_accuracies.append(train_accuracy)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Display results\n",
    "    print(f'Epoch: {epoch} | Train Loss: {train_loss} | Train Accuracy: {train_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot training curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 1500 validation examples: 94 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in valloader:\n",
    "        inputs, labels = data\n",
    "        \n",
    "        inputs = inputs.float()\n",
    "        labels = labels.float()\n",
    "        \n",
    "        # calculate outputs by running inputs through the network\n",
    "        outputs = network(inputs)\n",
    "        \n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        # Ww convert back from one-hot to integer for checking accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == torch.argmax(labels, dim=1)).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the {len(valloader.dataset)} validation examples: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 1772 test examples: 53 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        inputs, labels = data\n",
    "        \n",
    "        inputs = inputs.float()\n",
    "        labels = labels.float()\n",
    "        \n",
    "        # calculate outputs by running inputs through the network\n",
    "        outputs = network(inputs)\n",
    "        \n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        # Ww convert back from one-hot to integer for checking accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == torch.argmax(labels, dim=1)).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the {len(testloader.dataset)} test examples: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
