{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "ERROR! Session/line number was not unique in database. History logging moved to new session 86\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from utils.preprocess import getData\n",
    "from utils.validate import validate\n",
    "from networks.cnn import BasicCNN\n",
    "from networks.cnn import OptimizedCNN\n",
    "from networks.cnn import OptimizedCNNV2\n",
    "from networks.cnn import DeepCNN\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.test_accuracy import test\n",
    "from utils.test_accuracy import compute_test_outputs\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Load, Preprocess Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_valid, y_valid, X_test, y_test, person_train, person_valid, person_test, mapToOriginal = getData(lib='torch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shape of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: torch.Size([6960, 22, 250, 1])\n",
      "Valid data shape: torch.Size([1500, 22, 250, 1])\n",
      "Test data shape: torch.Size([1772, 22, 250, 1])\n",
      "Training target shape: torch.Size([6960, 4])\n",
      "Valid target shape: torch.Size([1500, 4])\n",
      "Test target shape: torch.Size([1772, 4])\n",
      "Training Persons shape: torch.Size([6960, 1])\n",
      "Valid Persons shape: torch.Size([1500, 1])\n",
      "Test Persons shape: torch.Size([443, 1])\n"
     ]
    }
   ],
   "source": [
    "print('Training data shape: {}'.format(X_train.shape))\n",
    "print('Valid data shape: {}'.format(X_valid.shape))\n",
    "print('Test data shape: {}'.format(X_test.shape))\n",
    "\n",
    "print('Training target shape: {}'.format(y_train.shape))\n",
    "print('Valid target shape: {}'.format(y_valid.shape))\n",
    "print('Test target shape: {}'.format(y_test.shape))\n",
    "\n",
    "print('Training Persons shape: {}'.format(person_train.shape))\n",
    "print('Valid Persons shape: {}'.format(person_valid.shape))\n",
    "print('Test Persons shape: {}'.format(person_test.shape))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Dataset and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "trainset = torch.utils.data.TensorDataset(X_train,y_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=0)\n",
    "\n",
    "# Shuffle is set to false for validation and test sets since no training is done on them, all we do is evaluate.\n",
    "valset =  torch.utils.data.TensorDataset(X_valid, y_valid)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=0)\n",
    "\n",
    "testset = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1,\n",
    "                                          shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BasicCNN(\n",
      "  (conv1): Conv2d(22, 25, kernel_size=(10, 1), stride=(1, 1), padding=same)\n",
      "  (pool1): MaxPool2d(kernel_size=(3, 1), stride=1, padding=(1, 0), dilation=1, ceil_mode=False)\n",
      "  (batchnorm1): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout1): Dropout(p=0.4, inplace=False)\n",
      "  (conv2): Conv2d(25, 50, kernel_size=(10, 1), stride=(1, 1), padding=same)\n",
      "  (pool2): MaxPool2d(kernel_size=(3, 1), stride=1, padding=(1, 0), dilation=1, ceil_mode=False)\n",
      "  (batchnorm2): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout2): Dropout(p=0.4, inplace=False)\n",
      "  (conv3): Conv2d(50, 100, kernel_size=(10, 1), stride=(1, 1), padding=same)\n",
      "  (pool3): MaxPool2d(kernel_size=(3, 1), stride=1, padding=(1, 0), dilation=1, ceil_mode=False)\n",
      "  (batchnorm3): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout3): Dropout(p=0.4, inplace=False)\n",
      "  (conv4): Conv2d(100, 200, kernel_size=(10, 1), stride=(1, 1), padding=same)\n",
      "  (pool4): MaxPool2d(kernel_size=(3, 1), stride=1, padding=(1, 0), dilation=1, ceil_mode=False)\n",
      "  (batchnorm4): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout4): Dropout(p=0.4, inplace=False)\n",
      "  (affine): Linear(in_features=50000, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Initialize the network\n",
    "network = BasicCNN()\n",
    "print(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/siddharthladha/opt/anaconda3/envs/finalProject/lib/python3.9/site-packages/torch/nn/modules/conv.py:442: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at  ../aten/src/ATen/native/Convolution.cpp:647.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "\t -- Train Loss: 3.2646478523901843 | Train Accuracy: 42.47126436781609\n",
      "\t -- Val Loss: 2.778337443868319 | Val Accuracy: 47.4\n",
      "Epoch: 1\n",
      "\t -- Train Loss: 2.6728703833501273 | Train Accuracy: 56.867816091954026\n",
      "\t -- Val Loss: 1.7206367899974186 | Val Accuracy: 62.6\n",
      "Epoch: 2\n",
      "\t -- Train Loss: 1.8336009678490666 | Train Accuracy: 67.05459770114942\n",
      "\t -- Val Loss: 1.5905541256070137 | Val Accuracy: 66.0\n",
      "Epoch: 3\n",
      "\t -- Train Loss: 1.3209000076722661 | Train Accuracy: 73.72126436781609\n",
      "\t -- Val Loss: 1.4152166247367859 | Val Accuracy: 69.66666666666667\n",
      "Epoch: 4\n",
      "\t -- Train Loss: 0.9155816865076712 | Train Accuracy: 80.17241379310344\n",
      "\t -- Val Loss: 0.9820176747937998 | Val Accuracy: 78.0\n",
      "Epoch: 5\n",
      "\t -- Train Loss: 0.6672043010182337 | Train Accuracy: 83.47701149425288\n",
      "\t -- Val Loss: 0.7940009571611881 | Val Accuracy: 79.73333333333333\n",
      "Epoch: 6\n",
      "\t -- Train Loss: 0.5427009351209763 | Train Accuracy: 86.6235632183908\n",
      "\t -- Val Loss: 0.3672515243912737 | Val Accuracy: 88.8\n",
      "Epoch: 7\n",
      "\t -- Train Loss: 0.43244507799454784 | Train Accuracy: 88.36206896551724\n",
      "\t -- Val Loss: 0.4982106393824021 | Val Accuracy: 86.4\n",
      "Epoch: 8\n",
      "\t -- Train Loss: 0.32816122103174894 | Train Accuracy: 90.76149425287356\n",
      "\t -- Val Loss: 0.46165689763923484 | Val Accuracy: 87.2\n",
      "Epoch: 9\n",
      "\t -- Train Loss: 0.28240288233538285 | Train Accuracy: 91.63793103448276\n",
      "\t -- Val Loss: 0.5187592646107078 | Val Accuracy: 86.2\n",
      "Epoch: 10\n",
      "\t -- Train Loss: 0.22005971476709077 | Train Accuracy: 93.23275862068965\n",
      "\t -- Val Loss: 0.281579543525974 | Val Accuracy: 92.26666666666667\n",
      "Epoch: 11\n",
      "\t -- Train Loss: 0.19060137367234864 | Train Accuracy: 94.00862068965517\n",
      "\t -- Val Loss: 0.3356117137397329 | Val Accuracy: 91.66666666666667\n",
      "Epoch: 12\n",
      "\t -- Train Loss: 0.16903095487334313 | Train Accuracy: 95.05747126436782\n",
      "\t -- Val Loss: 0.28745704299459857 | Val Accuracy: 91.8\n",
      "Epoch: 13\n",
      "\t -- Train Loss: 0.1565982841741328 | Train Accuracy: 94.97126436781609\n",
      "\t -- Val Loss: 0.20150791915754476 | Val Accuracy: 93.86666666666666\n",
      "Epoch: 14\n",
      "\t -- Train Loss: 0.1316709002103964 | Train Accuracy: 95.67528735632185\n",
      "\t -- Val Loss: 0.11588963627582416 | Val Accuracy: 97.0\n",
      "Epoch: 15\n",
      "\t -- Train Loss: 0.11331336578320063 | Train Accuracy: 96.30747126436782\n",
      "\t -- Val Loss: 0.15602193627273664 | Val Accuracy: 95.93333333333334\n",
      "Epoch: 16\n",
      "\t -- Train Loss: 0.11588848456569495 | Train Accuracy: 96.22126436781609\n",
      "\t -- Val Loss: 0.1341211834611992 | Val Accuracy: 96.6\n",
      "Epoch: 17\n",
      "\t -- Train Loss: 0.0981991069643425 | Train Accuracy: 96.70977011494253\n",
      "\t -- Val Loss: 0.20762616458038488 | Val Accuracy: 95.26666666666667\n",
      "Epoch: 18\n",
      "\t -- Train Loss: 0.09124696907083239 | Train Accuracy: 97.11206896551724\n",
      "\t -- Val Loss: 0.16936830795990923 | Val Accuracy: 96.06666666666666\n",
      "Epoch: 19\n",
      "\t -- Train Loss: 0.09757980883736676 | Train Accuracy: 96.86781609195403\n",
      "\t -- Val Loss: 0.15910534219195446 | Val Accuracy: 96.53333333333333\n"
     ]
    }
   ],
   "source": [
    "# Select loss criterion\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.001)\n",
    "\n",
    "# Setup learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "# Train the network\n",
    "num_epochs = 20\n",
    "\n",
    "# Store the loss\n",
    "stats = {\n",
    "    'train_accuracies': [],\n",
    "    'train_losses': [],\n",
    "    'val_accuracies': [],\n",
    "    'val_losses': []\n",
    "}\n",
    "\n",
    "for epoch in range(num_epochs): # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.float()\n",
    "        labels = labels.float()\n",
    "  \n",
    "        # forward pass\n",
    "        outputs = network(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "\n",
    "        # backward + optimize\n",
    "        loss.backward() # backward to get gradient values\n",
    "        \n",
    "        optimizer.step() # does the update\n",
    "    \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # accumulate loss\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Make prediction for batch\n",
    "        _, predicted = outputs.max(1)\n",
    "        \n",
    "        # Store accuracy for batch\n",
    "        # WE convert back from one-hot to integer for checking accuracy\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(torch.argmax(labels, dim=1)).sum().item()\n",
    "        \n",
    "    # Store accuracy,loss for epoch\n",
    "    train_loss=running_loss/len(trainloader)\n",
    "    train_accuracy=100.*correct/total\n",
    "    \n",
    "    # At the end of each epoch, calculate validation accuracy\n",
    "    \n",
    "    # Set the network in eval mode since we're not training here\n",
    "    network.eval()\n",
    "    \n",
    "     # Turn gradient computation off\n",
    "    with torch.no_grad():\n",
    "        val_accuracy, val_loss = validate(network, valloader, criterion)\n",
    "    \n",
    "    # Set the network back in training mode\n",
    "    network.train()\n",
    "    \n",
    "    stats['train_accuracies'].append(train_accuracy)\n",
    "    stats['train_losses'].append(train_loss)\n",
    "    stats['val_accuracies'].append(val_accuracy)\n",
    "    stats['val_losses'].append(val_loss)\n",
    "    \n",
    "    \n",
    "    # Display results\n",
    "    print(f'Epoch: {epoch}')\n",
    "    print(f'\\t -- Train Loss: {train_loss} | Train Accuracy: {train_accuracy}')\n",
    "    print(f'\\t -- Val Loss: {val_loss} | Val Accuracy: {val_accuracy}')\n",
    "    \n",
    "    # At the end of each epoch, schedule the learning rate decay\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot training curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(np.arange(num_epochs), stats['train_losses'], label='Training')\n",
    "plt.plot(np.arange(num_epochs), stats['val_losses'], label='Validation')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.arange(num_epochs), stats['train_accuracies'], label='Training')\n",
    "plt.plot(np.arange(num_epochs), stats['val_accuracies'], label='Validation')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:79] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 443 test examples: 57.78781038374717 %\n"
     ]
    }
   ],
   "source": [
    "# Set the network in eval mode since we're not training here\n",
    "\n",
    "network.eval()\n",
    "outputs = None\n",
    "\n",
    " # Turn gradient computation off\n",
    "with torch.no_grad():\n",
    "    outputs = compute_test_outputs(network,testloader,y_test)\n",
    "    \n",
    "# Set the network back in training mode\n",
    "network.train()\n",
    "    \n",
    "    \n",
    "test_accuracy = test(outputs, mapToOriginal['y_test'])\n",
    "\n",
    "num_examples = mapToOriginal['y_test'].size\n",
    "\n",
    "\n",
    "print(f'Accuracy of the network on the {num_examples} test examples: {test_accuracy} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model for inference or further training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"saved-models/basicCNN2.pth\"\n",
    "state = {\n",
    "    'epoch': 5,\n",
    "    'state_dict': network.state_dict(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "}\n",
    "torch.save(state, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing the model\n",
    "\n",
    "Now, let us try to optimize the model by varying the configuration of the layers, and using hyperparameter tuning to determine the best filter size, among other hyper paramaters\n",
    "\n",
    "From lectures, we have learnt that it is often useful to have multiple convolutional layers before a pooling layer to add complexity to our model.\n",
    "\n",
    "In addition, it is also advisable to put the batch-normalization layer immediately after the convolutional layer. (from lecture).\n",
    "\n",
    "Let us try these ideas below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OptimizedCNN(\n",
      "  (conv1): Conv2d(22, 25, kernel_size=(10, 1), stride=(1, 1), padding=same)\n",
      "  (batchnorm1): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout1): Dropout(p=0.4, inplace=False)\n",
      "  (conv2): Conv2d(25, 50, kernel_size=(10, 1), stride=(1, 1), padding=same)\n",
      "  (batchnorm2): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout2): Dropout(p=0.4, inplace=False)\n",
      "  (pool1): MaxPool2d(kernel_size=(3, 1), stride=1, padding=(1, 0), dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(50, 100, kernel_size=(10, 1), stride=(1, 1), padding=same)\n",
      "  (batchnorm3): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout3): Dropout(p=0.4, inplace=False)\n",
      "  (conv4): Conv2d(100, 200, kernel_size=(10, 1), stride=(1, 1), padding=same)\n",
      "  (batchnorm4): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout4): Dropout(p=0.4, inplace=False)\n",
      "  (pool2): MaxPool2d(kernel_size=(3, 1), stride=1, padding=(1, 0), dilation=1, ceil_mode=False)\n",
      "  (affine): Linear(in_features=50000, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Initialize the network\n",
    "network = OptimizedCNN()\n",
    "print(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "\t -- Train Loss: 7.163239629990464 | Train Accuracy: 31.882183908045977\n",
      "\t -- Val Loss: 3.117256780465444 | Val Accuracy: 32.53333333333333\n",
      "Epoch: 1\n",
      "\t -- Train Loss: 5.40870305266949 | Train Accuracy: 39.798850574712645\n",
      "\t -- Val Loss: 27.874557971954346 | Val Accuracy: 28.666666666666668\n",
      "Epoch: 2\n",
      "\t -- Train Loss: 4.09515522051295 | Train Accuracy: 43.9080459770115\n",
      "\t -- Val Loss: 43.28024927775065 | Val Accuracy: 30.733333333333334\n",
      "Epoch: 3\n",
      "\t -- Train Loss: 2.933387707132812 | Train Accuracy: 47.701149425287355\n",
      "\t -- Val Loss: 12341.8271484375 | Val Accuracy: 24.466666666666665\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m  \u001b[38;5;66;03m# Turn gradient computation off\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 67\u001b[0m     val_accuracy, val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Set the network back in training mode\u001b[39;00m\n\u001b[1;32m     70\u001b[0m network\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/Desktop/ECE C147/Project/project-code/utils/validate.py:18\u001b[0m, in \u001b[0;36mvalidate\u001b[0;34m(network, valloader, criterion)\u001b[0m\n\u001b[1;32m     15\u001b[0m inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     16\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m---> 18\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     20\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/finalProject/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/ECE C147/Project/project-code/networks/cnn.py:137\u001b[0m, in \u001b[0;36mOptimizedCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# [conv-batchnorm-relu-dropout-conv-batchnorm-relu-dropout]\u001b[39;00m\n\u001b[1;32m    136\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout3(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatchnorm3(F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(x))))\n\u001b[0;32m--> 137\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout4(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatchnorm4(F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv4\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)))\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m# pool\u001b[39;00m\n\u001b[1;32m    141\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool2(x)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/finalProject/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/finalProject/lib/python3.9/site-packages/torch/nn/modules/conv.py:446\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 446\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/finalProject/lib/python3.9/site-packages/torch/nn/modules/conv.py:442\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    440\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    441\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 442\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Select loss criterion\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.001)\n",
    "\n",
    "# Setup learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "# Train the network\n",
    "num_epochs = 10\n",
    "\n",
    "# Store the loss\n",
    "stats = {\n",
    "    'train_accuracies': [],\n",
    "    'train_losses': [],\n",
    "    'val_accuracies': [],\n",
    "    'val_losses': []\n",
    "}\n",
    "\n",
    "for epoch in range(num_epochs): # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.float()\n",
    "        labels = labels.float()\n",
    "  \n",
    "        # forward pass\n",
    "        outputs = network(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "\n",
    "        # backward + optimize\n",
    "        loss.backward() # backward to get gradient values\n",
    "        \n",
    "        optimizer.step() # does the update\n",
    "    \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # accumulate loss\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Make prediction for batch\n",
    "        _, predicted = outputs.max(1)\n",
    "        \n",
    "        # Store accuracy for batch\n",
    "        # WE convert back from one-hot to integer for checking accuracy\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(torch.argmax(labels, dim=1)).sum().item()\n",
    "        \n",
    "    # Store accuracy,loss for epoch\n",
    "    train_loss=running_loss/len(trainloader)\n",
    "    train_accuracy=100.*correct/total\n",
    "    \n",
    "    # At the end of each epoch, calculate validation accuracy\n",
    "    \n",
    "    # Set the network in eval mode since we're not training here\n",
    "    network.eval()\n",
    "    \n",
    "     # Turn gradient computation off\n",
    "    with torch.no_grad():\n",
    "        val_accuracy, val_loss = validate(network, valloader, criterion)\n",
    "    \n",
    "    # Set the network back in training mode\n",
    "    network.train()\n",
    "    \n",
    "    stats['train_accuracies'].append(train_accuracy)\n",
    "    stats['train_losses'].append(train_loss)\n",
    "    stats['val_accuracies'].append(val_accuracy)\n",
    "    stats['val_losses'].append(val_loss)\n",
    "    \n",
    "    \n",
    "    # Display results\n",
    "    print(f'Epoch: {epoch}')\n",
    "    print(f'\\t -- Train Loss: {train_loss} | Train Accuracy: {train_accuracy}')\n",
    "    print(f'\\t -- Val Loss: {val_loss} | Val Accuracy: {val_accuracy}')\n",
    "    \n",
    "    # At the end of each epoch, schedule the learning rate decay\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:79] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 443 test examples: 40.85778781038375 %\n"
     ]
    }
   ],
   "source": [
    "network.eval()\n",
    "outputs = None\n",
    "\n",
    " # Turn gradient computation off\n",
    "with torch.no_grad():\n",
    "    outputs = compute_test_outputs(network,testloader,y_test)\n",
    "    \n",
    "# Set the network back in training mode\n",
    "network.train()\n",
    "    \n",
    "    \n",
    "test_accuracy = test(outputs, mapToOriginal['y_test'])\n",
    "\n",
    "num_examples = mapToOriginal['y_test'].size\n",
    "\n",
    "\n",
    "print(f'Accuracy of the network on the {num_examples} test examples: {test_accuracy} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing Again\n",
    "\n",
    "However, after testing such an architecture, we find that it performs worse than the BasicCNN above.\n",
    "\n",
    "In that case, we turn to a new idea: Using the same BasicCNN architecture, but changing varying filter sizes and numbers, along with dropout probabilities as shown below:\n",
    "\n",
    "Since we are performing much worse on the test set than the train and validation set, we can assume that our model needs to generalize to unseen data better. Thus, we should explore a more aggressive dropout value. The above model does this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OptimizedCNNV2(\n",
      "  (conv1): Conv2d(22, 25, kernel_size=(10, 1), stride=(1, 1), padding=same)\n",
      "  (pool1): MaxPool2d(kernel_size=(3, 1), stride=1, padding=(1, 0), dilation=1, ceil_mode=False)\n",
      "  (batchnorm1): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout1): Dropout(p=0.6, inplace=False)\n",
      "  (conv2): Conv2d(25, 50, kernel_size=(10, 1), stride=(1, 1), padding=same)\n",
      "  (pool2): MaxPool2d(kernel_size=(3, 1), stride=1, padding=(1, 0), dilation=1, ceil_mode=False)\n",
      "  (batchnorm2): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout2): Dropout(p=0.6, inplace=False)\n",
      "  (conv3): Conv2d(50, 100, kernel_size=(10, 1), stride=(1, 1), padding=same)\n",
      "  (pool3): MaxPool2d(kernel_size=(3, 1), stride=1, padding=(1, 0), dilation=1, ceil_mode=False)\n",
      "  (batchnorm3): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout3): Dropout(p=0.6, inplace=False)\n",
      "  (conv4): Conv2d(100, 200, kernel_size=(10, 1), stride=(1, 1), padding=same)\n",
      "  (pool4): MaxPool2d(kernel_size=(3, 1), stride=1, padding=(1, 0), dilation=1, ceil_mode=False)\n",
      "  (batchnorm4): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout4): Dropout(p=0.6, inplace=False)\n",
      "  (affine): Linear(in_features=50000, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Initialize the network\n",
    "network = OptimizedCNNV2(dropout=0.6)\n",
    "print(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "\t -- Train Loss: 3.3448479788019023 | Train Accuracy: 36.5948275862069\n",
      "\t -- Val Loss: 3.936804632345835 | Val Accuracy: 33.266666666666666\n",
      "Epoch: 1\n",
      "\t -- Train Loss: 3.791455459157261 | Train Accuracy: 44.89942528735632\n",
      "\t -- Val Loss: 3.4451233943303428 | Val Accuracy: 42.333333333333336\n",
      "Epoch: 2\n",
      "\t -- Train Loss: 3.117615513845321 | Train Accuracy: 51.910919540229884\n",
      "\t -- Val Loss: 3.5258830785751343 | Val Accuracy: 48.13333333333333\n",
      "Epoch: 3\n",
      "\t -- Train Loss: 2.824068847052548 | Train Accuracy: 55.40229885057471\n",
      "\t -- Val Loss: 2.5542559574047723 | Val Accuracy: 52.6\n",
      "Epoch: 4\n",
      "\t -- Train Loss: 2.255259522604286 | Train Accuracy: 61.30747126436781\n",
      "\t -- Val Loss: 1.7741254170735676 | Val Accuracy: 60.4\n",
      "Epoch: 5\n",
      "\t -- Train Loss: 2.0331569735063324 | Train Accuracy: 62.98850574712644\n",
      "\t -- Val Loss: 2.6911876996358237 | Val Accuracy: 52.86666666666667\n",
      "Epoch: 6\n",
      "\t -- Train Loss: 1.8464201918435752 | Train Accuracy: 66.02011494252874\n",
      "\t -- Val Loss: 1.7625255386034648 | Val Accuracy: 56.53333333333333\n",
      "Epoch: 7\n",
      "\t -- Train Loss: 1.6757942930274052 | Train Accuracy: 67.44252873563218\n",
      "\t -- Val Loss: 2.353191708525022 | Val Accuracy: 52.0\n",
      "Epoch: 8\n",
      "\t -- Train Loss: 1.4333283906682917 | Train Accuracy: 70.28735632183908\n",
      "\t -- Val Loss: 2.7038501600424447 | Val Accuracy: 54.733333333333334\n",
      "Epoch: 9\n",
      "\t -- Train Loss: 1.379156284376022 | Train Accuracy: 71.25\n",
      "\t -- Val Loss: 2.122082238396009 | Val Accuracy: 57.06666666666667\n"
     ]
    }
   ],
   "source": [
    "# Select loss criterion\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.001)\n",
    "\n",
    "# Setup learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "# Train the network\n",
    "num_epochs = 10\n",
    "\n",
    "# Store the loss\n",
    "stats = {\n",
    "    'train_accuracies': [],\n",
    "    'train_losses': [],\n",
    "    'val_accuracies': [],\n",
    "    'val_losses': []\n",
    "}\n",
    "\n",
    "for epoch in range(num_epochs): # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.float()\n",
    "        labels = labels.float()\n",
    "  \n",
    "        # forward pass\n",
    "        outputs = network(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "\n",
    "        # backward + optimize\n",
    "        loss.backward() # backward to get gradient values\n",
    "        \n",
    "        optimizer.step() # does the update\n",
    "    \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # accumulate loss\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Make prediction for batch\n",
    "        _, predicted = outputs.max(1)\n",
    "        \n",
    "        # Store accuracy for batch\n",
    "        # WE convert back from one-hot to integer for checking accuracy\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(torch.argmax(labels, dim=1)).sum().item()\n",
    "        \n",
    "    # Store accuracy,loss for epoch\n",
    "    train_loss=running_loss/len(trainloader)\n",
    "    train_accuracy=100.*correct/total\n",
    "    \n",
    "    # At the end of each epoch, calculate validation accuracy\n",
    "    \n",
    "    # Set the network in eval mode since we're not training here\n",
    "    network.eval()\n",
    "    \n",
    "     # Turn gradient computation off\n",
    "    with torch.no_grad():\n",
    "        val_accuracy, val_loss = validate(network, valloader, criterion)\n",
    "    \n",
    "    # Set the network back in training mode\n",
    "    network.train()\n",
    "    \n",
    "    stats['train_accuracies'].append(train_accuracy)\n",
    "    stats['train_losses'].append(train_loss)\n",
    "    stats['val_accuracies'].append(val_accuracy)\n",
    "    stats['val_losses'].append(val_loss)\n",
    "    \n",
    "    \n",
    "    # Display results\n",
    "    print(f'Epoch: {epoch}')\n",
    "    print(f'\\t -- Train Loss: {train_loss} | Train Accuracy: {train_accuracy}')\n",
    "    print(f'\\t -- Val Loss: {val_loss} | Val Accuracy: {val_accuracy}')\n",
    "    \n",
    "    # At the end of each epoch, schedule the learning rate decay\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 443 test examples: 25.056433408577877 %\n"
     ]
    }
   ],
   "source": [
    "network.eval()\n",
    "outputs = None\n",
    "\n",
    " # Turn gradient computation off\n",
    "with torch.no_grad():\n",
    "    outputs = compute_test_outputs(network,testloader,y_test)\n",
    "    \n",
    "# Set the network back in training mode\n",
    "network.train()\n",
    "    \n",
    "    \n",
    "test_accuracy = test(outputs, mapToOriginal['y_test'])\n",
    "\n",
    "num_examples = mapToOriginal['y_test'].size\n",
    "\n",
    "\n",
    "print(f'Accuracy of the network on the {num_examples} test examples: {test_accuracy} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the aggressive dropout value does not seem to work. Let us now try changing the convolutional filter size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OptimizedCNNV2(\n",
      "  (conv1): Conv2d(22, 25, kernel_size=(20, 1), stride=(1, 1), padding=same)\n",
      "  (pool1): MaxPool2d(kernel_size=(3, 1), stride=1, padding=(1, 0), dilation=1, ceil_mode=False)\n",
      "  (batchnorm1): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout1): Dropout(p=0.4, inplace=False)\n",
      "  (conv2): Conv2d(25, 50, kernel_size=(20, 1), stride=(1, 1), padding=same)\n",
      "  (pool2): MaxPool2d(kernel_size=(3, 1), stride=1, padding=(1, 0), dilation=1, ceil_mode=False)\n",
      "  (batchnorm2): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout2): Dropout(p=0.4, inplace=False)\n",
      "  (conv3): Conv2d(50, 100, kernel_size=(20, 1), stride=(1, 1), padding=same)\n",
      "  (pool3): MaxPool2d(kernel_size=(3, 1), stride=1, padding=(1, 0), dilation=1, ceil_mode=False)\n",
      "  (batchnorm3): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout3): Dropout(p=0.4, inplace=False)\n",
      "  (conv4): Conv2d(100, 200, kernel_size=(20, 1), stride=(1, 1), padding=same)\n",
      "  (pool4): MaxPool2d(kernel_size=(3, 1), stride=1, padding=(1, 0), dilation=1, ceil_mode=False)\n",
      "  (batchnorm4): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout4): Dropout(p=0.4, inplace=False)\n",
      "  (affine): Linear(in_features=50000, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "network = OptimizedCNNV2(dropout=0.4, filter_size = (20,1))\n",
    "print(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "\t -- Train Loss: 3.7248429409954524 | Train Accuracy: 37.42816091954023\n",
      "\t -- Val Loss: 3.0248947739601135 | Val Accuracy: 40.666666666666664\n",
      "Epoch: 1\n",
      "\t -- Train Loss: 2.826133613192707 | Train Accuracy: 50.67528735632184\n",
      "\t -- Val Loss: 2.754539499680201 | Val Accuracy: 49.4\n",
      "Epoch: 2\n",
      "\t -- Train Loss: 2.000901311909387 | Train Accuracy: 60.79022988505747\n",
      "\t -- Val Loss: 1.693855106830597 | Val Accuracy: 62.46666666666667\n",
      "Epoch: 3\n",
      "\t -- Train Loss: 1.3590182645605244 | Train Accuracy: 69.55459770114942\n",
      "\t -- Val Loss: 1.1361853207151096 | Val Accuracy: 70.73333333333333\n",
      "Epoch: 4\n",
      "\t -- Train Loss: 1.0640239805803386 | Train Accuracy: 75.47413793103448\n",
      "\t -- Val Loss: 1.1719834382335346 | Val Accuracy: 70.53333333333333\n",
      "Epoch: 5\n",
      "\t -- Train Loss: 0.7456813836043034 | Train Accuracy: 81.02011494252874\n",
      "\t -- Val Loss: 0.6136873755604029 | Val Accuracy: 82.73333333333333\n",
      "Epoch: 6\n",
      "\t -- Train Loss: 0.5266175070487031 | Train Accuracy: 84.94252873563218\n",
      "\t -- Val Loss: 0.4612935396532218 | Val Accuracy: 85.13333333333334\n",
      "Epoch: 7\n",
      "\t -- Train Loss: 0.4576059871037072 | Train Accuracy: 86.95402298850574\n",
      "\t -- Val Loss: 0.4385887148479621 | Val Accuracy: 86.93333333333334\n",
      "Epoch: 8\n",
      "\t -- Train Loss: 0.35014931420120626 | Train Accuracy: 89.39655172413794\n",
      "\t -- Val Loss: 0.2704994510859251 | Val Accuracy: 92.13333333333334\n",
      "Epoch: 9\n",
      "\t -- Train Loss: 0.2447361615101952 | Train Accuracy: 92.5\n",
      "\t -- Val Loss: 0.19918819734205803 | Val Accuracy: 93.4\n"
     ]
    }
   ],
   "source": [
    "# Select loss criterion\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.001)\n",
    "\n",
    "# Setup learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "# Train the network\n",
    "num_epochs = 10\n",
    "\n",
    "# Store the loss\n",
    "stats = {\n",
    "    'train_accuracies': [],\n",
    "    'train_losses': [],\n",
    "    'val_accuracies': [],\n",
    "    'val_losses': []\n",
    "}\n",
    "\n",
    "for epoch in range(num_epochs): # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.float()\n",
    "        labels = labels.float()\n",
    "  \n",
    "        # forward pass\n",
    "        outputs = network(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "\n",
    "        # backward + optimize\n",
    "        loss.backward() # backward to get gradient values\n",
    "        \n",
    "        optimizer.step() # does the update\n",
    "    \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # accumulate loss\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Make prediction for batch\n",
    "        _, predicted = outputs.max(1)\n",
    "        \n",
    "        # Store accuracy for batch\n",
    "        # WE convert back from one-hot to integer for checking accuracy\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(torch.argmax(labels, dim=1)).sum().item()\n",
    "        \n",
    "    # Store accuracy,loss for epoch\n",
    "    train_loss=running_loss/len(trainloader)\n",
    "    train_accuracy=100.*correct/total\n",
    "    \n",
    "    # At the end of each epoch, calculate validation accuracy\n",
    "    \n",
    "    # Set the network in eval mode since we're not training here\n",
    "    network.eval()\n",
    "    \n",
    "     # Turn gradient computation off\n",
    "    with torch.no_grad():\n",
    "        val_accuracy, val_loss = validate(network, valloader, criterion)\n",
    "    \n",
    "    # Set the network back in training mode\n",
    "    network.train()\n",
    "    \n",
    "    stats['train_accuracies'].append(train_accuracy)\n",
    "    stats['train_losses'].append(train_loss)\n",
    "    stats['val_accuracies'].append(val_accuracy)\n",
    "    stats['val_losses'].append(val_loss)\n",
    "    \n",
    "    \n",
    "    # Display results\n",
    "    print(f'Epoch: {epoch}')\n",
    "    print(f'\\t -- Train Loss: {train_loss} | Train Accuracy: {train_accuracy}')\n",
    "    print(f'\\t -- Val Loss: {val_loss} | Val Accuracy: {val_accuracy}')\n",
    "    \n",
    "    # At the end of each epoch, schedule the learning rate decay\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 443 test examples: 54.176072234762984 %\n"
     ]
    }
   ],
   "source": [
    "network.eval()\n",
    "outputs = None\n",
    "\n",
    " # Turn gradient computation off\n",
    "with torch.no_grad():\n",
    "    outputs = compute_test_outputs(network,testloader,y_test)\n",
    "    \n",
    "# Set the network back in training mode\n",
    "network.train()\n",
    "    \n",
    "    \n",
    "test_accuracy = test(outputs, mapToOriginal['y_test'])\n",
    "\n",
    "num_examples = mapToOriginal['y_test'].size\n",
    "\n",
    "\n",
    "print(f'Accuracy of the network on the {num_examples} test examples: {test_accuracy} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This network performs about as well as our initial basic network. Let's now try decreasing the convolutional filter size, and increasing the depth of our network. (to maintain the same receptive field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepCNN(\n",
      "  (conv1): Conv2d(22, 25, kernel_size=(5, 1), stride=(1, 1), padding=same)\n",
      "  (pool1): MaxPool2d(kernel_size=(3, 1), stride=1, padding=(1, 0), dilation=1, ceil_mode=False)\n",
      "  (batchnorm1): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout1): Dropout(p=0.4, inplace=False)\n",
      "  (conv2): Conv2d(25, 50, kernel_size=(5, 1), stride=(1, 1), padding=same)\n",
      "  (pool2): MaxPool2d(kernel_size=(3, 1), stride=1, padding=(1, 0), dilation=1, ceil_mode=False)\n",
      "  (batchnorm2): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout2): Dropout(p=0.4, inplace=False)\n",
      "  (conv3): Conv2d(50, 100, kernel_size=(5, 1), stride=(1, 1), padding=same)\n",
      "  (pool3): MaxPool2d(kernel_size=(3, 1), stride=1, padding=(1, 0), dilation=1, ceil_mode=False)\n",
      "  (batchnorm3): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout3): Dropout(p=0.4, inplace=False)\n",
      "  (conv4): Conv2d(100, 200, kernel_size=(5, 1), stride=(1, 1), padding=same)\n",
      "  (pool4): MaxPool2d(kernel_size=(3, 1), stride=1, padding=(1, 0), dilation=1, ceil_mode=False)\n",
      "  (batchnorm4): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout4): Dropout(p=0.4, inplace=False)\n",
      "  (conv5): Conv2d(200, 150, kernel_size=(5, 1), stride=(1, 1), padding=same)\n",
      "  (pool5): MaxPool2d(kernel_size=(3, 1), stride=1, padding=(1, 0), dilation=1, ceil_mode=False)\n",
      "  (batchnorm5): BatchNorm2d(150, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout5): Dropout(p=0.4, inplace=False)\n",
      "  (conv6): Conv2d(150, 50, kernel_size=(5, 1), stride=(1, 1), padding=same)\n",
      "  (pool6): MaxPool2d(kernel_size=(3, 1), stride=1, padding=(1, 0), dilation=1, ceil_mode=False)\n",
      "  (batchnorm6): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout6): Dropout(p=0.4, inplace=False)\n",
      "  (affine): Linear(in_features=12500, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "network = DeepCNN(dropout=0.4, filter_size = (5,1))\n",
    "print(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "\t -- Train Loss: 1.6492772058609428 | Train Accuracy: 40.60344827586207\n",
      "\t -- Val Loss: 1.2389489139119785 | Val Accuracy: 47.46666666666667\n",
      "Epoch: 1\n",
      "\t -- Train Loss: 1.3457449723821167 | Train Accuracy: 55.201149425287355\n",
      "\t -- Val Loss: 1.3005520502726238 | Val Accuracy: 49.4\n",
      "Epoch: 2\n",
      "\t -- Train Loss: 1.0438917328458313 | Train Accuracy: 63.160919540229884\n",
      "\t -- Val Loss: 1.1931135058403015 | Val Accuracy: 52.53333333333333\n",
      "Epoch: 3\n",
      "\t -- Train Loss: 0.8821134583665691 | Train Accuracy: 69.066091954023\n",
      "\t -- Val Loss: 0.8955751533309618 | Val Accuracy: 62.86666666666667\n",
      "Epoch: 4\n",
      "\t -- Train Loss: 0.699154916979851 | Train Accuracy: 74.18103448275862\n",
      "\t -- Val Loss: 0.7968036358555158 | Val Accuracy: 69.53333333333333\n",
      "Epoch: 5\n",
      "\t -- Train Loss: 0.5880448974053795 | Train Accuracy: 77.77298850574712\n",
      "\t -- Val Loss: 0.6987029674152533 | Val Accuracy: 73.2\n",
      "Epoch: 6\n",
      "\t -- Train Loss: 0.5064953859519521 | Train Accuracy: 81.16379310344827\n",
      "\t -- Val Loss: 0.6284670444826285 | Val Accuracy: 74.13333333333334\n",
      "Epoch: 7\n",
      "\t -- Train Loss: 0.4482110051113531 | Train Accuracy: 83.2183908045977\n",
      "\t -- Val Loss: 0.6232262750466665 | Val Accuracy: 74.73333333333333\n",
      "Epoch: 8\n",
      "\t -- Train Loss: 0.39188598834593363 | Train Accuracy: 85.1580459770115\n",
      "\t -- Val Loss: 0.4671938878794511 | Val Accuracy: 81.46666666666667\n",
      "Epoch: 9\n",
      "\t -- Train Loss: 0.36729744814951487 | Train Accuracy: 86.12068965517241\n",
      "\t -- Val Loss: 0.5277232403556505 | Val Accuracy: 79.86666666666666\n"
     ]
    }
   ],
   "source": [
    "# Select loss criterion\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.001)\n",
    "\n",
    "# Setup learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "# Train the network\n",
    "num_epochs = 10\n",
    "\n",
    "# Store the loss\n",
    "stats = {\n",
    "    'train_accuracies': [],\n",
    "    'train_losses': [],\n",
    "    'val_accuracies': [],\n",
    "    'val_losses': []\n",
    "}\n",
    "\n",
    "for epoch in range(num_epochs): # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.float()\n",
    "        labels = labels.float()\n",
    "  \n",
    "        # forward pass\n",
    "        outputs = network(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "\n",
    "        # backward + optimize\n",
    "        loss.backward() # backward to get gradient values\n",
    "        \n",
    "        optimizer.step() # does the update\n",
    "    \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # accumulate loss\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Make prediction for batch\n",
    "        _, predicted = outputs.max(1)\n",
    "        \n",
    "        # Store accuracy for batch\n",
    "        # WE convert back from one-hot to integer for checking accuracy\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(torch.argmax(labels, dim=1)).sum().item()\n",
    "        \n",
    "    # Store accuracy,loss for epoch\n",
    "    train_loss=running_loss/len(trainloader)\n",
    "    train_accuracy=100.*correct/total\n",
    "    \n",
    "    # At the end of each epoch, calculate validation accuracy\n",
    "    \n",
    "    # Set the network in eval mode since we're not training here\n",
    "    network.eval()\n",
    "    \n",
    "     # Turn gradient computation off\n",
    "    with torch.no_grad():\n",
    "        val_accuracy, val_loss = validate(network, valloader, criterion)\n",
    "    \n",
    "    # Set the network back in training mode\n",
    "    network.train()\n",
    "    \n",
    "    stats['train_accuracies'].append(train_accuracy)\n",
    "    stats['train_losses'].append(train_loss)\n",
    "    stats['val_accuracies'].append(val_accuracy)\n",
    "    stats['val_losses'].append(val_loss)\n",
    "    \n",
    "    \n",
    "    # Display results\n",
    "    print(f'Epoch: {epoch}')\n",
    "    print(f'\\t -- Train Loss: {train_loss} | Train Accuracy: {train_accuracy}')\n",
    "    print(f'\\t -- Val Loss: {val_loss} | Val Accuracy: {val_accuracy}')\n",
    "    \n",
    "    # At the end of each epoch, schedule the learning rate decay\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 443 test examples: 54.85327313769752 %\n"
     ]
    }
   ],
   "source": [
    "network.eval()\n",
    "outputs = None\n",
    "\n",
    " # Turn gradient computation off\n",
    "with torch.no_grad():\n",
    "    outputs = compute_test_outputs(network,testloader,y_test)\n",
    "    \n",
    "# Set the network back in training mode\n",
    "network.train()\n",
    "    \n",
    "    \n",
    "test_accuracy = test(outputs, mapToOriginal['y_test'])\n",
    "\n",
    "num_examples = mapToOriginal['y_test'].size\n",
    "\n",
    "\n",
    "print(f'Accuracy of the network on the {num_examples} test examples: {test_accuracy} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even this deeper CNN with smaller filter size does not perform better than the original Basic CNN. At this point, we concede that the Basic CNN architecture is the most suitable out of all the ones we tested, and we vary its number of filters and learning rate in an attempt to achieve even greater performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
